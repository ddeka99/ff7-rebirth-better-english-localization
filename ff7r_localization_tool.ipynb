{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FF7R Translation & Localization Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import traceback\n",
    "import re\n",
    "import pandas as pd\n",
    "from openai import AzureOpenAI\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize Azure OpenAI client with key-based authentication\n",
    "endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")  \n",
    "deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")  \n",
    "subscription_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "   \n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=endpoint,  \n",
    "    api_key=subscription_key,  \n",
    "    api_version=\"2024-05-01-preview\",  \n",
    ")\n",
    "\n",
    "# File to be translated\n",
    "file_name = \"4000-MIDGR_TxtRes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test the client\n",
    "# prompt = \"Describe Tifa Lockhart from Final Fantasy VII in a json format\"\n",
    "\n",
    "# completion = client.chat.completions.create(\n",
    "#     model=deployment,\n",
    "#     messages=[\n",
    "#         {\"role\": \"system\", \"content\": \"You are an expert in the lore and story of the game Final Fantasy VII which includes the orignal game, remakes and spin-offs.\"},\n",
    "#         {\"role\": \"user\", \"content\": prompt}\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# # Print the response\n",
    "# print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dialogue_file(file_path):\n",
    "    # Load CSV file\n",
    "    df = pd.read_csv(file_path, encoding=\"utf-8\")\n",
    "    \n",
    "    # Remove metadata row where id is 'language'\n",
    "    df = df[df[\"id\"] != \"language\"]\n",
    "    \n",
    "    # Remove rows where both 'sub_id' and 'text' are empty\n",
    "    df = df.dropna(subset=[\"sub_id\", \"text\"], how=\"all\")\n",
    "    \n",
    "    # Ensure 'text' column is treated as a string and replace NaN with empty string\n",
    "    df[\"text\"] = df[\"text\"].fillna(\"\").astype(str)\n",
    "    \n",
    "    # Initialize a list to store structured dialogues\n",
    "    dialogues = []\n",
    "    \n",
    "    # Count occurrences of each ID\n",
    "    id_counts = df[\"id\"].value_counts()\n",
    "    \n",
    "    # Iterate through unique IDs\n",
    "    for unique_id, count in id_counts.items():\n",
    "        rows = df[df[\"id\"] == unique_id]\n",
    "        \n",
    "        if count == 2:\n",
    "            # If there are two rows, determine speaker and dialogue\n",
    "            speaker_row = rows[rows[\"sub_id\"] == \"ACTOR\"]\n",
    "            dialogue_row = rows[rows[\"sub_id\"].isna()]\n",
    "            \n",
    "            if not speaker_row.empty and not dialogue_row.empty:\n",
    "                speaker = speaker_row.iloc[0][\"text\"].strip()\n",
    "                dialogue = dialogue_row.iloc[0][\"text\"].strip()\n",
    "                \n",
    "                if speaker and dialogue:\n",
    "                    dialogues.append({\"id\": unique_id, \"speaker\": speaker, \"dialogue\": dialogue})\n",
    "        \n",
    "        elif count == 1:\n",
    "            # If there is only one row, assume it's an NPC/system dialogue\n",
    "            dialogue = rows.iloc[0][\"text\"].strip()\n",
    "            if dialogue:\n",
    "                dialogues.append({\"id\": unique_id, \"speaker\": \"NPC\", \"dialogue\": dialogue})\n",
    "    \n",
    "    # Convert structured dialogues into a DataFrame\n",
    "    return pd.DataFrame(dialogues)\n",
    "\n",
    "# Process English and Japanese files\n",
    "en_file_path = f\"./testing/ModifiedExports/{file_name}.csv\"\n",
    "jp_file_path = f\"./testing/ModifiedExports/{file_name}_jp.csv\"\n",
    "\n",
    "en_dialogue_df = process_dialogue_file(en_file_path)\n",
    "jp_dialogue_df = process_dialogue_file(jp_file_path)\n",
    "\n",
    "# Merge English and Japanese dialogues using a left join\n",
    "merged_dialogue_df = en_dialogue_df.merge(jp_dialogue_df, on=\"id\", how=\"left\", suffixes=(\"_en\", \"_jp\"))\n",
    "\n",
    "# Remove the speaker_jp column since we want to translate dialogue only\n",
    "merged_dialogue_df = merged_dialogue_df.drop(columns=[\"speaker_jp\"])\n",
    "merged_dialogue_df.to_csv(f\"./testing/ModifiedExports/{file_name}_merged.csv\", index=False)\n",
    "\n",
    "print(f\"Relevant dialogues to be translated in {file_name}: {len(merged_dialogue_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_json_output(ai_response):\n",
    "    # If the response contains ```json, extract the content within\n",
    "    match = re.search(r\"```json\\s*(.*?)\\s*```\", ai_response, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1)  # Extract only the JSON part\n",
    "    return ai_response  # Return as-is if no backticks are found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the merged dialogue CSV file\n",
    "merged_file_path = f\"./testing/ModifiedExports/{file_name}_merged.csv\"\n",
    "merged_df = pd.read_csv(merged_file_path, encoding=\"utf-8\")\n",
    "\n",
    "# Sort by ID\n",
    "merged_df = merged_df.sort_values(\"id\")\n",
    "\n",
    "# Extract unique scene identifiers\n",
    "scene_ids = sorted(set(merged_df[\"id\"].str.split(\"_\").apply(lambda x: \"_\".join(x[:-3]))))\n",
    "\n",
    "# Store translations for all scenes\n",
    "all_translations = []\n",
    "\n",
    "for scene_id in scene_ids:\n",
    "    try:\n",
    "        scene_df = merged_df[merged_df[\"id\"].str.startswith(scene_id)].copy()\n",
    "        \n",
    "        # Create the prompt for AI translation\n",
    "        prompt = f\"\"\"\n",
    "        You are an expert translator specializing in Japanese-to-English localization for video games. Your task is to provide a faithful translation of the following dialogue from Final Fantasy VII Rebirth in a valid JSON format.\n",
    "        \n",
    "        Please translate while maintaining:\n",
    "        - The original tone and context.\n",
    "        - Character personality and speech style.\n",
    "        - Natural English phrasing.\n",
    "        \n",
    "        Return the results in a structured JSON format with the following structure:\n",
    "        {{\n",
    "            \"scene_id\": \"{scene_id}\",\n",
    "            \"translations\": [\n",
    "                {{\"id\": \"<original_id>\", \"translation\": \"<your improved English translation>\"}},\n",
    "                ...\n",
    "            ]\n",
    "        }}\n",
    "        \n",
    "        Here is the Japanese dialogue along with its official English localization which you can refer for additional context:\n",
    "        \"\"\"\n",
    "        \n",
    "        for _, row in scene_df.iterrows():\n",
    "            prompt += f\"\\nID: {row['id']}\"\n",
    "            prompt += f\"\\n{row['speaker_en']} (JP): {row['dialogue_jp']}\"\n",
    "            prompt += f\"\\n{row['speaker_en']} (EN): {row['dialogue_en']}\\n\"\n",
    "        \n",
    "        prompt += \"\\nPlease provide only the JSON output formatted as specified.\" \n",
    "        \n",
    "        # Ping the client\n",
    "        completion = client.chat.completions.create(\n",
    "            model=deployment,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert translator specializing in Final Fantasy VII localization.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=4096\n",
    "        )\n",
    "        \n",
    "        # Parse the JSON response safely\n",
    "        ai_response = completion.choices[0].message.content\n",
    "        ai_response_clean = clean_json_output(ai_response)\n",
    "        translated_data = json.loads(ai_response_clean)\n",
    "        all_translations.append(translated_data)\n",
    "\n",
    "        # Save progress after each scene\n",
    "        with open(f\"./testing/translations_backup_{file_name}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(all_translations, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        # Small delay to avoid hitting rate limits\n",
    "        time.sleep(2)\n",
    "\n",
    "        # Log progress\n",
    "        print(f\"Completed scene {scene_id}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Capture error details\n",
    "        time_now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "        error_details = {\n",
    "            \"timestamp\": time_now,\n",
    "            \"scene_id\": scene_id,\n",
    "            \"error_message\": str(e),\n",
    "            \"traceback\": traceback.format_exc(),\n",
    "            \"prompt_used\": prompt,\n",
    "            \"response_received\": ai_response\n",
    "        }\n",
    "\n",
    "        error_log_file = f\"./testing/logs/error_{file_name}_{scene_id}.json\"\n",
    "\n",
    "        # Save error details to a file\n",
    "        with open(error_log_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(error_details, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        print(f\"Error processing scene {scene_id}. Logged details to {error_log_file}\")\n",
    "\n",
    "# Convert all translations to DataFrame\n",
    "final_translation_df = pd.DataFrame([t for scene in all_translations for t in scene[\"translations\"]])\n",
    "\n",
    "# Save the final translations to a CSV file\n",
    "final_translation_df.to_csv(f\"./testing/ModifiedExports/{file_name}_translated.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the original and translated CSV files\n",
    "df_original = pd.read_csv(f\"./testing/ModifiedExports/{file_name}.csv\")\n",
    "df_translated = pd.read_csv(f\"./testing/ModifiedExports/{file_name}_translated.csv\")\n",
    "\n",
    "# Merge original and translated datasets on 'id'\n",
    "df_merged = df_original.merge(df_translated, on=\"id\", how=\"left\")\n",
    "\n",
    "# Identify dialogue rows that have a matching translation (sub_id is NaN, text is not blank, and translation is not blank)\n",
    "dialogue_mask = df_merged[\"sub_id\"].isna() & df_merged[\"text\"].notna() & df_merged[\"translation\"].notna()\n",
    "\n",
    "# Replace text column only for matching instances with a translation\n",
    "df_merged.loc[dialogue_mask, \"text\"] = df_merged.loc[dialogue_mask, \"translation\"]\n",
    "\n",
    "# Drop the extra 'translation' column after updating\n",
    "df_merged.drop(columns=[\"translation\"], inplace=True)\n",
    "\n",
    "# Save the updated file\n",
    "df_merged.to_csv(f\"./testing/ModifiedExports/{file_name}_updated.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
