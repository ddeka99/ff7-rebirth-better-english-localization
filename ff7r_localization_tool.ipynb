{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FF7R Translation & Localization Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import traceback\n",
    "import re\n",
    "import pandas as pd\n",
    "from openai import AzureOpenAI\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize Azure OpenAI client with key-based authentication\n",
    "endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")  \n",
    "deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")  \n",
    "subscription_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "   \n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=endpoint,  \n",
    "    api_key=subscription_key,  \n",
    "    api_version=\"2024-05-01-preview\",  \n",
    ")\n",
    "\n",
    "# File to be translated\n",
    "file_name = \"4000-MIDGR_TxtRes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test the client\n",
    "# prompt = \"Describe Tifa Lockhart from Final Fantasy VII in a json format\"\n",
    "\n",
    "# completion = client.chat.completions.create(\n",
    "#     model=deployment,\n",
    "#     messages=[\n",
    "#         {\"role\": \"system\", \"content\": \"You are an expert in the lore and story of the game Final Fantasy VII which includes the orignal game, remakes and spin-offs.\"},\n",
    "#         {\"role\": \"user\", \"content\": prompt}\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# # Print the response\n",
    "# print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dialogue_file(file_path):\n",
    "    # Load CSV file\n",
    "    df = pd.read_csv(file_path, encoding=\"utf-8\")\n",
    "    \n",
    "    # Remove metadata row where id is 'language'\n",
    "    df = df[df[\"id\"] != \"language\"]\n",
    "    \n",
    "    # Remove rows where both 'sub_id' and 'text' are empty\n",
    "    df = df.dropna(subset=[\"sub_id\", \"text\"], how=\"all\")\n",
    "    \n",
    "    # Ensure 'text' column is treated as a string and replace NaN with empty string\n",
    "    df[\"text\"] = df[\"text\"].fillna(\"\").astype(str)\n",
    "    \n",
    "    # Initialize a list to store structured dialogues\n",
    "    dialogues = []\n",
    "    \n",
    "    # Count occurrences of each ID\n",
    "    id_counts = df[\"id\"].value_counts()\n",
    "    \n",
    "    # Iterate through unique IDs\n",
    "    for unique_id, count in id_counts.items():\n",
    "        rows = df[df[\"id\"] == unique_id]\n",
    "        \n",
    "        if count == 2:\n",
    "            # If there are two rows, determine speaker and dialogue\n",
    "            speaker_row = rows[rows[\"sub_id\"] == \"ACTOR\"]\n",
    "            dialogue_row = rows[rows[\"sub_id\"].isna()]\n",
    "            \n",
    "            if not speaker_row.empty and not dialogue_row.empty:\n",
    "                speaker = speaker_row.iloc[0][\"text\"].strip()\n",
    "                dialogue = dialogue_row.iloc[0][\"text\"].strip()\n",
    "                \n",
    "                if speaker and dialogue:\n",
    "                    dialogues.append({\"id\": unique_id, \"speaker\": speaker, \"dialogue\": dialogue})\n",
    "        \n",
    "        elif count == 1:\n",
    "            # If there is only one row, assume it's an NPC/system dialogue\n",
    "            dialogue = rows.iloc[0][\"text\"].strip()\n",
    "            if dialogue:\n",
    "                dialogues.append({\"id\": unique_id, \"speaker\": \"NPC\", \"dialogue\": dialogue})\n",
    "    \n",
    "    # Convert structured dialogues into a DataFrame\n",
    "    return pd.DataFrame(dialogues)\n",
    "\n",
    "# Process English and Japanese files\n",
    "en_file_path = f\"./testing/ModifiedExports/{file_name}.csv\"\n",
    "jp_file_path = f\"./testing/ModifiedExports/{file_name}_jp.csv\"\n",
    "\n",
    "en_dialogue_df = process_dialogue_file(en_file_path)\n",
    "jp_dialogue_df = process_dialogue_file(jp_file_path)\n",
    "\n",
    "# Merge English and Japanese dialogues using a left join\n",
    "merged_dialogue_df = en_dialogue_df.merge(jp_dialogue_df, on=\"id\", how=\"left\", suffixes=(\"_en\", \"_jp\"))\n",
    "\n",
    "# Remove the speaker_jp column since we want to translate dialogue only\n",
    "merged_dialogue_df = merged_dialogue_df.drop(columns=[\"speaker_jp\"])\n",
    "merged_dialogue_df.to_csv(f\"./testing/ModifiedExports/{file_name}_merged.csv\", index=False)\n",
    "\n",
    "print(f\"Relevant dialogues to be translated in {file_name}: {len(merged_dialogue_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sort_keys(id_str):\n",
    "    parts = id_str.split('_')\n",
    "    timestamp = int(parts[-3])  # Convert 3rd last part to integer\n",
    "    scene_id = '_'.join(parts[:-3])  # Everything before last three parts\n",
    "    return scene_id, timestamp\n",
    "\n",
    "# Sort dataframe using extracted keys\n",
    "merged_dialogue_df = merged_dialogue_df.sort_values(by=[\"id\"], key=lambda x: x.map(extract_sort_keys))\n",
    "merged_dialogue_df.to_csv(f\"./testing/ModifiedExports/{file_name}_merged_sorted.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_json_output(ai_response):\n",
    "    # If the response contains ```json, extract the content within\n",
    "    match = re.search(r\"```json\\s*(.*?)\\s*```\", ai_response, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1)  # Extract only the JSON part\n",
    "    return ai_response  # Return as-is if no backticks are found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the merged dialogue CSV file\n",
    "merged_file_path = f\"./testing/ModifiedExports/{file_name}_merged_sorted.csv\"\n",
    "merged_df = pd.read_csv(merged_file_path, encoding=\"utf-8\")\n",
    "\n",
    "# Extract unique scene identifiers\n",
    "# scene_ids = sorted(set(merged_df[\"id\"].str.split(\"_\").apply(lambda x: \"_\".join(x[:-3]))))\n",
    "scene_ids = ['$MIDGR_NPC_10_0100']\n",
    "\n",
    "# Store translations for all scenes\n",
    "all_translations = []\n",
    "\n",
    "for scene_id in scene_ids:\n",
    "    try:\n",
    "        # Filter the dataframe for the current scene\n",
    "        scene_df = merged_df[merged_df[\"id\"].str.startswith(scene_id)].copy()\n",
    "        \n",
    "        # Split the scene into chunks of 30 dialogues\n",
    "        max_dialogues_per_request = 25\n",
    "        num_chunks = (len(scene_df) + max_dialogues_per_request - 1) // max_dialogues_per_request  # Round up\n",
    "        \n",
    "        # Store translated chunks\n",
    "        scene_translations = {\n",
    "            \"scene_id\": scene_id,\n",
    "            \"translations\": []\n",
    "        }\n",
    "\n",
    "        for chunk_index in range(num_chunks):\n",
    "            chunk_df = scene_df.iloc[chunk_index * max_dialogues_per_request:(chunk_index + 1) * max_dialogues_per_request]\n",
    "            \n",
    "            # Construct the prompt for AI translation\n",
    "            prompt = f\"\"\"\n",
    "You are an expert translator specializing in Japanese-to-English localization for video games. Your task is to provide a faithful translation of the following dialogue from Final Fantasy VII Rebirth in a valid JSON format.\n",
    "\n",
    "Please translate while maintaining:\n",
    "- The original tone and context.\n",
    "- Character personality and speech style.\n",
    "- Natural English phrasing.\n",
    "\n",
    "Return the results in a structured JSON format with the following structure:\n",
    "{{\n",
    "    \"scene_id\": \"{scene_id}\",\n",
    "    \"translations\": [\n",
    "        {{\"id\": \"<original_id>\", \"translation\": \"<your improved English translation>\"}} \n",
    "        ...\n",
    "    ]\n",
    "}}\n",
    "\n",
    "Here is the Japanese dialogue along with its official English localization which you can refer for additional context:\n",
    "\"\"\"\n",
    "\n",
    "            for _, row in chunk_df.iterrows():\n",
    "                prompt += f\"\\nID: {row['id']}\"\n",
    "                prompt += f\"\\n{row['speaker_en']} (JP): {row['dialogue_jp']}\"\n",
    "                prompt += f\"\\n{row['speaker_en']} (EN): {row['dialogue_en']}\\n\"\n",
    "\n",
    "            prompt += \"\\nPlease provide only the JSON output formatted as specified.\"\n",
    "\n",
    "            # Ping the client\n",
    "            completion = client.chat.completions.create(\n",
    "                model=deployment,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an expert translator specializing in Final Fantasy VII localization.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                max_tokens=4096\n",
    "            )\n",
    "\n",
    "            # Parse the JSON response safely\n",
    "            ai_response = completion.choices[0].message.content\n",
    "            ai_response_clean = clean_json_output(ai_response)\n",
    "            translated_data = json.loads(ai_response_clean)\n",
    "\n",
    "            # Merge translations into the scene's full list\n",
    "            scene_translations[\"translations\"].extend(translated_data[\"translations\"])\n",
    "\n",
    "            # Small delay to avoid hitting rate limits\n",
    "            time.sleep(30)\n",
    "\n",
    "            # Log progress for each chunk\n",
    "            print(f\"Completed chunk {chunk_index + 1}/{num_chunks} for scene {scene_id}\")\n",
    "\n",
    "        # Append full scene translations\n",
    "        all_translations.append(scene_translations)\n",
    "\n",
    "        # Save progress after each scene\n",
    "        with open(f\"./testing/translations_backup_{file_name}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(all_translations, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        # Log progress for full scene completion\n",
    "        print(f\"Completed scene {scene_id}\")\n",
    "        # TBD: We need a percentage completion logger (number of scenes translated/total number of scenes or something similar)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Capture error details\n",
    "        time_now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "        error_details = {\n",
    "            \"timestamp\": time_now,\n",
    "            \"scene_id\": scene_id,\n",
    "            \"error_message\": str(e),\n",
    "            \"traceback\": traceback.format_exc(),\n",
    "            \"prompt_used\": prompt,\n",
    "            \"response_received\": ai_response\n",
    "        }\n",
    "\n",
    "        error_log_file = f\"./testing/logs/error_{file_name}_{scene_id}.json\"\n",
    "\n",
    "        # Save error details to a file\n",
    "        with open(error_log_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(error_details, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        print(f\"Error processing scene {scene_id}. Logged details to {error_log_file}\")\n",
    "\n",
    "# Convert all translations to DataFrame\n",
    "final_translation_df = pd.DataFrame([t for scene in all_translations for t in scene[\"translations\"]])\n",
    "\n",
    "# Save the final translations to a CSV file\n",
    "final_translation_df.to_csv(f\"./testing/ModifiedExports/{file_name}_translated.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the original and translated CSV files\n",
    "df_original = pd.read_csv(f\"./testing/ModifiedExports/{file_name}.csv\")\n",
    "df_translated = pd.read_csv(f\"./testing/ModifiedExports/{file_name}_translated.csv\")\n",
    "\n",
    "# Merge original and translated datasets on 'id'\n",
    "df_merged = df_original.merge(df_translated, on=\"id\", how=\"left\")\n",
    "\n",
    "# Identify dialogue rows that have a matching translation (sub_id is NaN, text is not blank, and translation is not blank)\n",
    "dialogue_mask = df_merged[\"sub_id\"].isna() & df_merged[\"text\"].notna() & df_merged[\"translation\"].notna()\n",
    "\n",
    "# Replace text column only for matching instances with a translation\n",
    "df_merged.loc[dialogue_mask, \"text\"] = df_merged.loc[dialogue_mask, \"translation\"]\n",
    "\n",
    "# Drop the extra 'translation' column after updating\n",
    "df_merged.drop(columns=[\"translation\"], inplace=True)\n",
    "\n",
    "# Save the updated file\n",
    "df_merged.to_csv(f\"./testing/ModifiedExports/{file_name}_updated.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Fixing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ai_response from ChatGPT\n",
    "ai_response = {\n",
    "    \n",
    "}\n",
    "\n",
    "ai_response = json.dumps(ai_response)\n",
    "translated_data = json.loads(ai_response)\n",
    "translated_data_df = pd.DataFrame(translated_data[\"translations\"])\n",
    "\n",
    "# Read the existing translation file\n",
    "existing_translation_df = pd.read_csv(f\"./testing/ModifiedExports/{file_name}_translated.csv\")\n",
    "\n",
    "# Check if id starting with scene_id already exists in the translation file\n",
    "if existing_translation_df[\"id\"].str.startswith(translated_data[\"scene_id\"]).any():\n",
    "    print(f\"Scene {translated_data['scene_id']} already exists in the translation file.\")\n",
    "# Check if number of translations in the response matches the number of unique IDs in the scene\n",
    "elif len(translated_data[\"translations\"]) != len(merged_dialogue_df[merged_dialogue_df[\"id\"].str.startswith(translated_data[\"scene_id\"])]):\n",
    "    print(f\"Number of translations in the response does not match the number of unique IDs in scene {translated_data['scene_id']}.\")\n",
    "    missing_ids = sorted(list(set(merged_dialogue_df[merged_dialogue_df[\"id\"].str.startswith(translated_data[\"scene_id\"])][\"id\"]) - set(translated_data_df[\"id\"])))\n",
    "else:\n",
    "    # Append the new translations to the existing translation file\n",
    "    existing_translation_df = pd.concat([existing_translation_df, translated_data_df], ignore_index=True)\n",
    "    # existing_translation_df.to_csv(f\"./testing/ModifiedExports/{file_name}_translated.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
