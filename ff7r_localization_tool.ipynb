{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FF7R Translation & Localization Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import traceback\n",
    "import re\n",
    "import pandas as pd\n",
    "from openai import AzureOpenAI\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize Azure OpenAI client with key-based authentication\n",
    "endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")  \n",
    "deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")  \n",
    "subscription_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "   \n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=endpoint,  \n",
    "    api_key=subscription_key,  \n",
    "    api_version=\"2024-05-01-preview\",  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run configs\n",
    "file_name = \"8300-CORLE_TxtRes\"\n",
    "RERUN_MODE = True  # Set rerun mode to False to translate all scenes or True to translate only error scenes\n",
    "RESPONSE_FROM_CHATGPT = True  # Set to True if the response is obtained from ChatG PT manually\n",
    "complete_investigation = False  # Set to True to resolve investigation for already existing scenes in translation\n",
    "chatgpt_response = {\n",
    "    \"scene_id\": \"COSTT_BOS_06_1100\",\n",
    "    \"translations\": [\n",
    "        {\"id\": \"COSTT_BOS_06_1100_0100_00_hjo\", \"translation\": \"Oh? Running away, are we?\"},\n",
    "        {\"id\": \"COSTT_BOS_06_1100_0110_00_hjo\", \"translation\": \"I see. So you still don't wish to reunite.\"},\n",
    "        {\"id\": \"COSTT_BOS_06_1100_0200_00_hjo\", \"translation\": \"There's nothing to fear. Come with me, and you can contribute to the evolution of mankind.\"},\n",
    "        {\"id\": \"COSTT_BOS_06_1100_0300_00_hjo\", \"translation\": \"Your friends seem lonely. Shall we make them call out to you with their screams? Heh heh!\"},\n",
    "        {\"id\": \"COSTT_BOS_06_1100_0400_00_hjo\", \"translation\": \"Heh heh... Truly fascinating. I must cut you open and examine every last cell.\"},\n",
    "        {\"id\": \"COSTT_BOS_06_1100_0500_00_hjo\", \"translation\": \"Why not surrender quietly, just like your pitiful brethren?\"},\n",
    "        {\"id\": \"COSTT_BOS_06_1100_0600_00_hjo\", \"translation\": \"How does it feel to be slowly cornered? Hmm?\"},\n",
    "        {\"id\": \"COSTT_BOS_06_1100_0700_00_hjo\", \"translation\": \"Are you running around because you want to? Heh heh!\"},\n",
    "        {\"id\": \"COSTT_BOS_06_1100_0800_00_hjo\", \"translation\": \"Are you really that afraid of coming with me?\"},\n",
    "        {\"id\": \"COSTT_BOS_06_1100_0900_00_cld\", \"translation\": \"This is bad...\"},\n",
    "        {\"id\": \"COSTT_BOS_06_1100_1000_00_cld\", \"translation\": \"Now's my chance!\"},\n",
    "        {\"id\": \"COSTT_BOS_06_1100_1100_00_cld\", \"translation\": \"Hit 'em all at once!\"},\n",
    "        {\"id\": \"COSTT_BOS_06_1100_1200_00_cld\", \"translation\": \"Can I hold out...?\"},\n",
    "        {\"id\": \"COSTT_BOS_06_1100_1300_00_cld\", \"translation\": \"Not going down that easy.\"},\n",
    "        {\"id\": \"COSTT_BOS_06_1100_1400_00_hjo\", \"translation\": \"You're a valuable specimen, but I have no choice. You'll contribute to science as a corpse.\"},\n",
    "        {\"id\": \"COSTT_BOS_06_1100_1500_00_hjo\", \"translation\": \"I have no desire to waste any more of my precious time on you.\"},\n",
    "        {\"id\": \"COSTT_BOS_06_1100_1600_00_hjo\", \"translation\": \"Do you understand? Brilliant scientists bear the responsibility of guiding foolish humanity toward evolution.\"},\n",
    "        {\"id\": \"COSTT_BOS_06_1100_1700_00_hjo\", \"translation\": \"Now, once I return to the company... Ah, yes. I must resume that experiment.\"},\n",
    "        {\"id\": \"COSTT_BOS_06_1100_1800_00_hjo\", \"translation\": \"The variety of specimens is excellent, but the real question is... can the host endure?\"},\n",
    "        {\"id\": \"COSTT_BOS_06_1100_1900_00_hjo\", \"translation\": \"Heh heh... The creation of a hero will soon become a reality.\"},\n",
    "        {\"id\": \"COSTT_BOS_06_1100_2000_00_hjo\", \"translation\": \"How dull... How much longer do you plan to drag this out?\"},\n",
    "        {\"id\": \"COSTT_BOS_06_1100_2100_00_hjo\", \"translation\": \"Enough with the pointless resistance.\"},\n",
    "        {\"id\": \"COSTT_BOS_06_1100_2200_00_hjo\", \"translation\": \"Oh? Not bad at all.\"},\n",
    "        {\"id\": \"COSTT_BOS_06_1100_2300_00_hjo\", \"translation\": \"Do give it your best effort.\"},\n",
    "        {\"id\": \"COSTT_BOS_06_1100_2400_00_hjo\", \"translation\": \"Heh... Is that the best you can do?\"},\n",
    "        {\"id\": \"COSTT_BOS_06_1100_2500_00_hjo\", \"translation\": \"Hmph... I'll give you a chance.\"},\n",
    "        {\"id\": \"COSTT_BOS_06_1100_2600_00_hjo\", \"translation\": \"Come now, hurry, hurry! Heh heh!\"},\n",
    "        {\"id\": \"COSTT_BOS_06_1100_2700_00_hjo\", \"translation\": \"You should consider yourself honored.\"},\n",
    "        {\"id\": \"COSTT_BOS_06_1100_2800_00_hjo\", \"translation\": \"Breathe it inâ€”deep into your lungs.\"},\n",
    "        {\"id\": \"COSTT_BOS_06_1100_2900_00_hjo\", \"translation\": \"Heh heh... How are you feeling?\"},\n",
    "        {\"id\": \"COSTT_BOS_06_1100_3000_00_hjo\", \"translation\": \"Heh heh heh... Think you can find a way out?\"},\n",
    "        {\"id\": \"COSTT_BOS_06_1100_3100_00_hjo\", \"translation\": \"Come now, show me your face twisted in fear.\"},\n",
    "        {\"id\": \"COSTT_BOS_06_1100_3200_00_hjo\", \"translation\": \"It's about time I reduced you to dust.\"},\n",
    "        {\"id\": \"COSTT_BOS_06_1100_3300_00_hjo\", \"translation\": \"I'll cut you down to your very cells.\"},\n",
    "        {\"id\": \"COSTT_BOS_06_1100_3400_00_hjo\", \"translation\": \"Heh heh... Go on, run around in vain!\"},\n",
    "        {\"id\": \"COSTT_BOS_06_1100_3500_00_hjo\", \"translation\": \"I do so enjoy toying with my prey.\"}\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test the client\n",
    "# prompt = \"Describe Tifa Lockhart from Final Fantasy VII in a json format\"\n",
    "\n",
    "# completion = client.chat.completions.create(\n",
    "#     model=deployment,\n",
    "#     messages=[\n",
    "#         {\"role\": \"system\", \"content\": \"You are an expert in the lore and story of the game Final Fantasy VII which includes the orignal game, remakes and spin-offs.\"},\n",
    "#         {\"role\": \"user\", \"content\": prompt}\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# # Print the response\n",
    "# print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dialogue Extraction and Processing\n",
    "\n",
    "Reads the game's dialogue files from \n",
    "- `f\"./testing/ModifiedExports/{file_name}.csv\"`\n",
    "- `f\"./testing/ModifiedExports/{file_name}_jp.csv\"`\n",
    "\n",
    "and processes them to\n",
    "- `f\"./testing/ModifiedExports/{file_name}_merged_sorted.csv\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dialogue_file(file_path):\n",
    "    # Load CSV file\n",
    "    df = pd.read_csv(file_path, encoding=\"utf-8\")\n",
    "    \n",
    "    # Remove metadata row where id is 'language'\n",
    "    df = df[df[\"id\"] != \"language\"]\n",
    "    \n",
    "    # Remove rows where both 'sub_id' and 'text' are empty\n",
    "    df = df.dropna(subset=[\"sub_id\", \"text\"], how=\"all\")\n",
    "    \n",
    "    # Ensure 'text' column is treated as a string and replace NaN with empty string\n",
    "    df[\"text\"] = df[\"text\"].fillna(\"\").astype(str)\n",
    "    \n",
    "    # Initialize a list to store structured dialogues\n",
    "    dialogues = []\n",
    "    \n",
    "    # Count occurrences of each ID\n",
    "    id_counts = df[\"id\"].value_counts()\n",
    "    \n",
    "    # Iterate through unique IDs\n",
    "    for unique_id, count in id_counts.items():\n",
    "        rows = df[df[\"id\"] == unique_id]\n",
    "        \n",
    "        if count == 2:\n",
    "            # If there are two rows, determine speaker and dialogue\n",
    "            speaker_row = rows[rows[\"sub_id\"] == \"ACTOR\"]\n",
    "            dialogue_row = rows[rows[\"sub_id\"].isna()]\n",
    "            \n",
    "            if not speaker_row.empty and not dialogue_row.empty:\n",
    "                speaker = speaker_row.iloc[0][\"text\"].strip()\n",
    "                dialogue = dialogue_row.iloc[0][\"text\"].strip()\n",
    "                \n",
    "                if speaker and dialogue:\n",
    "                    dialogues.append({\"id\": unique_id, \"speaker\": speaker, \"dialogue\": dialogue})\n",
    "        \n",
    "        elif count == 1:\n",
    "            # If there is only one row, assume it's an NPC/system dialogue\n",
    "            dialogue = rows.iloc[0][\"text\"].strip()\n",
    "            if dialogue:\n",
    "                dialogues.append({\"id\": unique_id, \"speaker\": \"NPC\", \"dialogue\": dialogue})\n",
    "    \n",
    "    # Convert structured dialogues into a DataFrame\n",
    "    return pd.DataFrame(dialogues)\n",
    "\n",
    "# Process English and Japanese files\n",
    "en_file_path = f\"./testing/ModifiedExports/{file_name}.csv\"\n",
    "jp_file_path = f\"./testing/ModifiedExports/{file_name}_jp.csv\"\n",
    "\n",
    "en_dialogue_df = process_dialogue_file(en_file_path)\n",
    "jp_dialogue_df = process_dialogue_file(jp_file_path)\n",
    "\n",
    "# Merge English and Japanese dialogues using a left join\n",
    "merged_dialogue_df = en_dialogue_df.merge(jp_dialogue_df, on=\"id\", how=\"left\", suffixes=(\"_en\", \"_jp\"))\n",
    "\n",
    "# Remove the speaker_jp column since we want to translate dialogue only\n",
    "merged_dialogue_df = merged_dialogue_df.drop(columns=[\"speaker_jp\"])\n",
    "merged_dialogue_df.to_csv(f\"./testing/ModifiedExports/{file_name}_merged.csv\", index=False)\n",
    "\n",
    "print(f\"Relevant dialogues to be translated in {file_name}: {len(merged_dialogue_df)}\")\n",
    "\n",
    "def extract_sort_keys(id_str):\n",
    "    parts = id_str.split('_')\n",
    "    timestamp = int(parts[-3])  # Convert 3rd last part to integer\n",
    "    scene_id = '_'.join(parts[:-3])  # Everything before last three parts\n",
    "    return scene_id, timestamp\n",
    "\n",
    "# Sort dataframe using extracted keys\n",
    "merged_dialogue_df = merged_dialogue_df.sort_values(by=[\"id\"], key=lambda x: x.map(extract_sort_keys))\n",
    "merged_dialogue_df.to_csv(f\"./testing/ModifiedExports/{file_name}_merged_sorted.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils\n",
    "\n",
    "- `clean_json_output`: Takes an `ai_response` string and cleans it up for loading as a JSON object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_json_output(ai_response):\n",
    "    # If the response contains ```json, extract the content within\n",
    "    match = re.search(r\"```json\\s*(.*?)\\s*```\", ai_response, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1)  # Extract only the JSON part\n",
    "    return ai_response  # Return as-is if no backticks are found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get scene ids from the error logs for re-run\n",
    "\n",
    "scene_ids will get stored into the list `error_scene_ids` for re-run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain error scene ids from the error log\n",
    "def get_error_scene_ids(file_name):\n",
    "    error_log_folder = \"./testing/logs\"\n",
    "    error_scene_ids = []\n",
    "\n",
    "    for error_file in os.listdir(error_log_folder):\n",
    "        if error_file.endswith(\".json\"):\n",
    "            with open(os.path.join(error_log_folder, error_file), \"r\", encoding=\"utf-8\") as f:\n",
    "                error_data = json.load(f)\n",
    "                error_scene_id = error_data[\"scene_id\"]\n",
    "                error_scene_ids.append(error_scene_id)\n",
    "\n",
    "    # Write the error scene ids to a text file\n",
    "    error_scene_ids_file = f\"./testing/logs/{file_name}_error_scene_ids.txt\"\n",
    "    with open(error_scene_ids_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for scene_id in error_scene_ids:\n",
    "            f.write(f\"{scene_id}\\n\")\n",
    "\n",
    "    # Delete the error log files\n",
    "    for error_file in os.listdir(error_log_folder):\n",
    "        if error_file.endswith(\".json\"):\n",
    "            os.remove(os.path.join(error_log_folder, error_file))\n",
    "\n",
    "    return error_scene_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI Workflow\n",
    "\n",
    "- Takes the JP and EN dialogues from the merged file `f\"./testing/ModifiedExports/{file_name}_merged_sorted.csv\"`, prepares the prompt and runs the AI model to get the translation.\n",
    "- Error logs get written to `f\"./testing/logs/error_{file_name}_{scene_id}.json\"`\n",
    "- Output translations for all the scenes that processed successfully get written to `f\"./testing/ModifiedExports/{file_name}_translated.csv\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the merged dialogue CSV file\n",
    "merged_file_path = f\"./testing/ModifiedExports/{file_name}_merged_sorted.csv\"\n",
    "merged_df = pd.read_csv(merged_file_path, encoding=\"utf-8\")\n",
    "\n",
    "if not RERUN_MODE:\n",
    "    scene_ids = sorted(set(merged_df[\"id\"].str.split(\"_\").apply(lambda x: \"_\".join(x[:-3]))))\n",
    "    print(f\"Translating {len(scene_ids)} scenes.\")\n",
    "else:\n",
    "    scene_ids = get_error_scene_ids(file_name).copy()\n",
    "    # scene_ids = ['$C3210_MAIN_0200', '$NIBLE_QST_03_1400', '$NIBLM_BOS_00_1100']\n",
    "    print(f\"Rerun mode enabled. Translating {len(scene_ids)} scenes.\")\n",
    "    print(scene_ids)\n",
    "\n",
    "# Store translations for all scenes\n",
    "all_translations = []\n",
    "\n",
    "for scene_id in scene_ids:\n",
    "    try:\n",
    "        # Filter the dataframe for the current scene\n",
    "        scene_df = merged_df[merged_df[\"id\"].str.startswith(scene_id)].copy()\n",
    "        \n",
    "        # Split the scene into chunks of fixed dialogues\n",
    "        max_dialogues_per_request = 10\n",
    "        num_chunks = (len(scene_df) + max_dialogues_per_request - 1) // max_dialogues_per_request  # Round up\n",
    "        \n",
    "        # Store translated chunks\n",
    "        scene_translations = {\n",
    "            \"scene_id\": scene_id,\n",
    "            \"translations\": []\n",
    "        }\n",
    "\n",
    "        for chunk_index in range(num_chunks):\n",
    "            chunk_df = scene_df.iloc[chunk_index * max_dialogues_per_request:(chunk_index + 1) * max_dialogues_per_request]\n",
    "            \n",
    "            # Construct the prompt for AI translation\n",
    "            prompt = f\"\"\"\n",
    "You are an expert Japanese-to-English translator specializing in video game localization, particularly for *Final Fantasy VII Rebirth*. Your task is to translate the following lines while following the guidelines provided below.\n",
    "\n",
    "### **Guidelines:**  \n",
    "- Ensure the translation remains faithful to the original Japanese, while making it sound natural in English.  \n",
    "- Maintain character-specific speech patterns, formality levels, and personality traits.  \n",
    "- The provided official English localization is included as a reference. Use it for context, but prioritize accuracy to the original Japanese text.  \n",
    "- If the official localization takes creative liberties, your translation should focus on capturing the original intent while still reading smoothly.\n",
    "\n",
    "### **Output Format (JSON):**  \n",
    "{{\n",
    "    \"scene_id\": \"{scene_id}\",\n",
    "    \"translations\": [\n",
    "        {{\"id\": \"<original_id>\", \"translation\": \"<your improved English translation>\"}} \n",
    "        ...\n",
    "    ]\n",
    "}}\n",
    "\n",
    "Here is the Japanese dialogue for a chunk of a scene along with its official English localization for reference:\n",
    "\"\"\"\n",
    "\n",
    "            for _, row in chunk_df.iterrows():\n",
    "                prompt += f\"\\nID: {row['id']}\"\n",
    "                prompt += f\"\\n{row['speaker_en']} (JP): {row['dialogue_jp']}\"\n",
    "                prompt += f\"\\n{row['speaker_en']} (EN): {row['dialogue_en']}\\n\"\n",
    "\n",
    "            prompt += \"\\nReturn the output in the specified **valid JSON format**\"\n",
    "\n",
    "\n",
    "            # Ping the client\n",
    "            completion = client.chat.completions.create(\n",
    "                model=deployment,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an expert translator specializing in Final Fantasy VII localization.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                max_tokens=4096\n",
    "            )\n",
    "\n",
    "            # Parse the JSON response safely\n",
    "            ai_response = completion.choices[0].message.content\n",
    "            ai_response_clean = clean_json_output(ai_response)\n",
    "            translated_data = json.loads(ai_response_clean)\n",
    "\n",
    "            # Merge translations into the scene's full list\n",
    "            scene_translations[\"translations\"].extend(translated_data[\"translations\"])\n",
    "\n",
    "            # Small delay to avoid hitting rate limits\n",
    "            time.sleep(2)\n",
    "\n",
    "            # Log progress for each chunk\n",
    "            print(f\"Completed chunk {chunk_index + 1}/{num_chunks} for scene {scene_id}\")\n",
    "\n",
    "        # Append full scene translations\n",
    "        all_translations.append(scene_translations)\n",
    "\n",
    "        # Save progress after each scene if not in rerun mode\n",
    "        if not RERUN_MODE:\n",
    "            with open(f\"./testing/translations_backup_{file_name}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(all_translations, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        # Log progress for full scene completion\n",
    "        print(f\"Completed scene {scene_id}\")\n",
    "        # TBD: We need a percentage completion logger (number of scenes translated/total number of scenes or something similar)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Capture error details\n",
    "        time_now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "        error_details = {\n",
    "            \"timestamp\": time_now,\n",
    "            \"scene_id\": scene_id,\n",
    "            \"error_message\": str(e),\n",
    "            \"traceback\": traceback.format_exc(),\n",
    "            \"prompt_used\": prompt,\n",
    "            \"response_received\": ai_response\n",
    "        }\n",
    "\n",
    "        error_log_file = f\"./testing/logs/error_{file_name}_{scene_id}.json\"\n",
    "\n",
    "        # Save error details to a file\n",
    "        with open(error_log_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(error_details, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        print(f\"Error processing scene {scene_id}. Logged details to {error_log_file}\")\n",
    "\n",
    "# Convert all translations to DataFrame\n",
    "final_translation_df = pd.DataFrame([t for scene in all_translations for t in scene[\"translations\"]])\n",
    "\n",
    "# Save the final translations to a CSV file if not in rerun mode\n",
    "if not RERUN_MODE:\n",
    "    final_translation_df.to_csv(f\"./testing/ModifiedExports/{file_name}_translated.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RESPONSE_FROM_CHATGPT and RERUN_MODE:\n",
    "    chatgpt_response_json = json.dumps(chatgpt_response, ensure_ascii=False, indent=4)\n",
    "    translated_data = json.loads(chatgpt_response_json)\n",
    "    final_translation_df = pd.DataFrame(translated_data[\"translations\"])\n",
    "    print(f\"Response from ChatGPT written to final_translation_df for scene {translated_data['scene_id']}\")\n",
    "\n",
    "else:\n",
    "    print(\"Response from ChatGPT skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Fixing\n",
    "\n",
    "- Suppose we did a rerun for specific scene ids, they will be stored in `final_translation_df`.\n",
    "- We review it and append them to the existing translations in `f\"./testing/ModifiedExports/{file_name}_translated.csv\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RERUN_MODE:\n",
    "    new_translations = final_translation_df.copy()\n",
    "\n",
    "    # Read the existing translation file\n",
    "    existing_translation_df = pd.read_csv(f\"./testing/ModifiedExports/{file_name}_translated.csv\")\n",
    "\n",
    "    # For all ids in the new translations, check if it already exists in the existing translations\n",
    "    new_ids = new_translations[\"id\"].unique()\n",
    "    existing_ids = existing_translation_df[\"id\"].unique()\n",
    "    already_existing_ids = set(new_ids).intersection(existing_ids)\n",
    "    if len(already_existing_ids) > 0:\n",
    "        print(\"New translations contain IDs that already exist in the existing translations. Please check for duplicates.\")\n",
    "    else:\n",
    "        print(\"No duplicate IDs found in the new translations. Appending to existing translations...\")\n",
    "        existing_translation_df = pd.concat([existing_translation_df, new_translations], ignore_index=True)\n",
    "        existing_translation_df.to_csv(f\"./testing/ModifiedExports/{file_name}_translated.csv\", index=False)\n",
    "else:\n",
    "    print(\"Rerun mode disabled. Skipping manual fixing.\")\n",
    "\n",
    "# Investigate already existing scene ids\n",
    "if RERUN_MODE:\n",
    "    print(\"-----------------------------------\")\n",
    "    print(\"Investigating scene ids present in both new and existing translations...\")\n",
    "\n",
    "    # Obtain all scene ids from the new translations\n",
    "    new_scene_ids = final_translation_df[\"id\"].str.split(\"_\").apply(lambda x: \"_\".join(x[:-3])).unique()\n",
    "    # Obtain all scene ids from the existing translations\n",
    "    existing_scene_ids = existing_translation_df[\"id\"].str.split(\"_\").apply(lambda x: \"_\".join(x[:-3])).unique()\n",
    "\n",
    "    # Find scene ids that are already present in the existing translations\n",
    "    common_scene_ids = set(new_scene_ids).intersection(existing_scene_ids)\n",
    "    print(f\"Common scene ids between new and existing translations: {len(common_scene_ids)}\")\n",
    "    print(common_scene_ids)\n",
    "\n",
    "    # Find scene ids that are present in the new translations but not in the existing translations\n",
    "    new_only_scene_ids = set(new_scene_ids) - set(existing_scene_ids)\n",
    "    print(f\"Scene ids present only in new translations: {len(new_only_scene_ids)}\")\n",
    "    print(new_only_scene_ids)\n",
    "else:\n",
    "    print(\"Rerun mode disabled. Skipping investigation.\")\n",
    "\n",
    "if RERUN_MODE:\n",
    "    print(\"-----------------------------------\")\n",
    "    if complete_investigation:\n",
    "        print(\"Resolving already existing scene ids...\")\n",
    "\n",
    "        # Filter out the scene ids that are already present in the existing translations\n",
    "        final_translation_df = final_translation_df[~final_translation_df[\"id\"].str.split(\"_\").apply(lambda x: \"_\".join(x[:-3])).isin(common_scene_ids)]\n",
    "\n",
    "        print(\"Scenes deleted from new translations:\")\n",
    "    else:\n",
    "        print(\"Skipping investigation resolution.\")\n",
    "else:\n",
    "    print(\"Rerun mode disabled. Skipping investigation resolution.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-Processing\n",
    "\n",
    "Writes the new translations in `f\"./testing/ModifiedExports/{file_name}_translated.csv\"` to the original dialogue file `f\"./testing/ModifiedExports/{file_name}.csv\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the original and translated CSV files\n",
    "df_original = pd.read_csv(f\"./testing/ModifiedExports/{file_name}.csv\")\n",
    "df_translated = pd.read_csv(f\"./testing/ModifiedExports/{file_name}_translated.csv\")\n",
    "\n",
    "# Merge original and translated datasets on 'id'\n",
    "df_merged = df_original.merge(df_translated, on=\"id\", how=\"left\")\n",
    "\n",
    "# Identify dialogue rows that have a matching translation (sub_id is NaN, text is not blank, and translation is not blank)\n",
    "dialogue_mask = df_merged[\"sub_id\"].isna() & df_merged[\"text\"].notna() & df_merged[\"translation\"].notna()\n",
    "\n",
    "# Replace text column only for matching instances with a translation\n",
    "df_merged.loc[dialogue_mask, \"text\"] = df_merged.loc[dialogue_mask, \"translation\"]\n",
    "\n",
    "# Drop the extra 'translation' column after updating\n",
    "df_merged.drop(columns=[\"translation\"], inplace=True)\n",
    "\n",
    "# Save the updated file\n",
    "df_merged.to_csv(f\"./testing/ModifiedExports/{file_name}_updated.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
